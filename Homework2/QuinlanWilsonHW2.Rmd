---
title: "Homework Assignment 2 PSTAT 131"
author: "Quinlan Wilson and Jack Guo (both 131)"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, echo=FALSE}
library(knitr)
library(tidyverse)
library(ISLR)
library(ROCR)
library(dplyr)
library(ggplot2)
library(MASS)

# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(
	fig.height = 5,
	fig.width = 7,
	warning = FALSE
)
options(digits = 4)

## indents are for indenting r code as formatted text
## They may need to be adjusted depending on your OS
# if your output looks odd, increase or decrease indent
indent1 = '    '
indent2 = '        '
indent3 = '            '
```

## Linear Regression

### (1.)

```{r}
glimpse(Auto)

Auto_clean <- Auto %>%
  dplyr::select(-contains("name")) %>%
  mutate(origin = as.factor(origin))

lmod <- lm(mpg ~ ., data = Auto_clean)
summary(lmod)
```

With a $0.01$ threshold we can reject the null hypothesis that there is
no linear association between mpg and displacement, weight, year, and
origin. While failing to reject that cylinders, horsepower, and
acceleration have no linear association with mpg.

### (2.)

```{r}
training_MSE <- mean((Auto$mpg - predict(lmod, Auto_clean))^2)
```

The training MSE is `r training_MSE` and we cannot calculate the test
MSE as the training set is the whole dataset so we have nothing to
compare with.

### (3.)

```{r}
car <- data.frame(
  cylinders = 4,
  displacement = 133,
  horsepower = 117,
  weight = 3250,
  acceleration = 29,
  year = 97,
  origin = factor(2) 
)


car_pre_mpg <- predict(lmod, newdata = car)
```

The gas mileage predicted for an European car with 4 cylinders,
displacement 133, horsepower of 117, weight of 3250, acceleration of 29,
built in the year 1997 is `r car_pre_mpg`.

### (4.)

```{r}
coef(lmod)
```

The difference in mpg between a Japanese car and the mpg of an American
car is 2.85323 and the difference between a European car and an American
car is 2.63000.

### (5.)

The change in mpg associated with a 30 unit increase in horsepower is
`r  coef(lmod)[4] * 30`.

```{r}
algae <- read.table("algaeBloom.txt", col.names=
                      c('season','size','speed','mxPH','mnO2','Cl','NO3','NH4',
                        'oPO4','PO4','Chla','a1','a2','a3','a4','a5','a6','a7'),
                    na = "XXXXXXX")
```

```{r}
algae.transformed <- algae %>% mutate_at(vars(4:11), funs(log(.)))
algae.transformed <- algae.transformed %>%
mutate_at(vars(4:11),funs(ifelse(is.na(.),median(.,na.rm=TRUE),.)))
# a1 == 0 means low
algae.transformed <- algae.transformed %>% mutate(a1 = factor(as.integer(a1 > 5), levels = c(0, 1)))
```

```{r}

calc_error_rate <- function(predicted.value, true.value) {
  return(mean(true.value != predicted.value))
}
```

```{r}
set.seed(1)
test.indices = sample(1:nrow(algae.transformed), 50)
algae.train=algae.transformed[-test.indices,]
algae.test=algae.transformed[test.indices,]
```



## Algae Classification using Discriminant Analysis

### (1.)

```{r}
lda_mod <- lda(a1 ~ ., data = algae.transformed, CV = T)


lda_class <- lda_mod$class

lda_error <- calc_error_rate(lda_class, algae.train$a1)
lda_post <- lda_mod$posterior

if (ncol(lda_post) == 1) {
  lda_post <- cbind("0" = 1 - lda_post, "1" = lda_post)
}

lda_probs <- lda_post[, "1"]

pred <- prediction(lda_mod$posterior[, "1"], algae.transformed$a1)
perf <- performance(pred, 'tpr', 'fpr')

plot(perf)
pred_lda <- prediction(lda_probs, algae.transformed$a1)
performance(pred_lda, "auc")@y.values[[1]]

```

### (2.)

```{r}
qda_mod <- qda(a1 ~ ., data = algae.transformed, CV = T)
qda_class <- qda_mod$class
qda_post <- qda_mod$posterior

if (ncol(qda_post) == 1) qda_post <- cbind("0" = 1 - qda_post, "1" = qda_post)

qda_probs <- qda_post[, "1"]

pred_qda <- prediction(qda_probs, algae.transformed$a1)
perf_qda <- performance(pred_qda, "tpr", "fpr")
auc_qda <- performance(pred_qda, "auc")@y.values[[1]]

plot(perf_qda)

auc_qda
```
LDA has a higher area under the ROC curve between the two models meaning it performs better at separating the two classes. LDA has the advantage of lower variance but in the case of different contrivances more bias is introduced.

## Fundamentals of the bootstrap

### (1.)

$$(1-\frac{1}{n})^n$$

### (2.)
```{r}
(1-1/1000)^1000
```

### (3.)

```{r}
n <- 1000

sample <- sample(1:n, size = n, replace = T)

missing <- n - length(unique(sample))

ratio <- missing / n 
ratio
```

